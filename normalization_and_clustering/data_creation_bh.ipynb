{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a comprehensive file on how the final train - test set was created -- Blake Hartung 11/16/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_data(row, norm_depths):\n",
    "    norm_values = list()\n",
    "    for depth in norm_depths:\n",
    "        mid = np.floor(depth).astype('int64')\n",
    "        perc_up = 1 - (depth - mid)\n",
    "        perc_down = 1 - perc_up\n",
    "        chla = (row.iloc[mid-1]*perc_down)+(row.iloc[mid]*perc_up)\n",
    "        norm_values.append(chla)\n",
    "    return norm_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROFILE_DATA_PATH = \"../data/profileData.csv\"\n",
    "SAT_DATA_PATH = \"../data/satData.csv\"\n",
    "\n",
    "profile_df = pd.read_csv(PROFILE_DATA_PATH)\n",
    "sat_df = pd.read_csv(SAT_DATA_PATH)\n",
    "\n",
    "def build_binned_data(profile_df, sat_df, save_path=None, return_df=True):\n",
    "    drop_cols = ['LT_SAT_SST_SD', 'LT_SAT_SST_MED', 'LT_SAT_CHL_SD', 'LT_SAT_CHL_MED', 'LT_SAT_BBP_SD', 'LT_SAT_BBP_MED']\n",
    "    # cut out outlying depth values\n",
    "    profile_df = profile_df[profile_df.PRES < 1001]\n",
    "    # bin the data using 100 bins and group it\n",
    "    out = pd.cut(profile_df.PRES, bins=100, labels=[i for i in range(100)])\n",
    "    profile_df['depth_bin'] = out\n",
    "\n",
    "    depth_profiles = profile_df[['float', 'cycleNumber', 'depth_bin', 'CHLA', 'BBP700']] \\\n",
    "        .groupby(['float', 'cycleNumber', 'depth_bin']).mean().reset_index().dropna()\n",
    "\n",
    "    df_depth = depth_profiles.merge(sat_df, on=['float', 'cycleNumber']).drop(drop_cols, axis=1).dropna()\n",
    "    df_depth['date'] = pd.to_datetime(df_depth.date, format='%Y-%m-%d %H:%M:%S')\n",
    "    # make sure we grab all bins\n",
    "    bins_to_use = [i for i in range(100)]\n",
    "    df = df_depth[df_depth.depth_bin.isin(bins_to_use)]\n",
    "    df[\"depth_bin\"] = df.loc[:, 'depth_bin'].astype(\"int64\").astype(\"category\")\n",
    "    # turn date into radians\n",
    "    df['date_doy'] = df.loc[:, 'date'].apply(lambda x: x.day_of_year)\n",
    "    df['date_doy_rad'] = df.loc[:, 'date_doy'] * (np.pi /182.625)\n",
    "\n",
    "    # round lat and lon to match new data\n",
    "    df['latitude'] = df.latitude.apply(lambda x: np.around(x, 3))\n",
    "    df['longitude'] = df.longitude.apply(lambda x: np.around(x, 3))\n",
    "\n",
    "    if save_path != None:\n",
    "        df.to_csv(save_path, index=False)\n",
    "    if return_df:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pic = pd.read_csv('../data/sat_pic_full_final.csv').drop(['sat_pic_day', 'sat_pic_8d', 'short_date'], axis=1)\n",
    "df_par = pd.read_csv('../data/par_full_final.csv').drop(['par_day', 'par_8d', 'short_date'], axis=1)\n",
    "df_aph = pd.read_csv('../data/aph_443_full_final.csv').drop(['aph_443_day', 'aph_443_8d', 'short_date'], axis=1)\n",
    "\n",
    "\n",
    "def build_pivoted_imputed_data(df_binned, df_pic, df_par, df_aph, save_path=None, return_df=True):\n",
    "    new_features_df = df_pic.merge(df_par, on=['latitude', 'longitude'])\n",
    "    new_features_df = new_features_df.merge(df_aph, on=['latitude', 'longitude'])\n",
    "\n",
    "    piv_df = df.merge(new_features_df, on=['latitude', 'longitude']).dropna().drop_duplicates()\n",
    "    pivoted_ls = list()\n",
    "    curr_float, curr_cycle = -1, -1\n",
    "    pivot_row = ''\n",
    "    for i, r in piv_df.iterrows():\n",
    "        # new cycle\n",
    "        if (curr_float != r.float) and (curr_cycle != r.cycleNumber):\n",
    "            pivoted_ls.append(pivot_row)\n",
    "            curr_float = r.float\n",
    "            curr_cycle = r.cycleNumber\n",
    "            pivot_row = {\n",
    "                'float': curr_float,\n",
    "                'cycleNumber': curr_cycle,\n",
    "                'latitude': r.latitude,\n",
    "                'longitude': r.longitude,\n",
    "                'date_doy_rad': r.date_doy_rad,\n",
    "                'sat_chl_month': r.MO_SAT_CHL,\n",
    "                'sat_sst_month': r.MO_SAT_SST,\n",
    "                'sat_par_month': r.par_month,\n",
    "                'sat_pic_month': r.sat_pic_month,\n",
    "                'sat_aph_443_month': r.aph_443_month\n",
    "            }\n",
    "            pivot_row['depth_bin_' + str(r.depth_bin)] = r.CHLA\n",
    "        # continuing from past cycle\n",
    "        elif curr_cycle != r.cycleNumber:\n",
    "            pivoted_ls.append(pivot_row)\n",
    "            curr_cycle = r.cycleNumber\n",
    "            pivot_row = {\n",
    "                'float': curr_float,\n",
    "                'cycleNumber': curr_cycle,\n",
    "                'latitude': r.latitude,\n",
    "                'longitude': r.longitude,\n",
    "                'date_doy_rad': r.date_doy_rad,\n",
    "                'sat_chl_month': r.MO_SAT_CHL,\n",
    "                'sat_sst_month': r.MO_SAT_SST,\n",
    "                'sat_par_month': r.par_month,\n",
    "                'sat_pic_month': r.sat_pic_month,\n",
    "                'sat_aph_443_month': r.aph_443_month\n",
    "            }\n",
    "            pivot_row['depth_bin_' + str(r.depth_bin)] = r.CHLA\n",
    "        else:\n",
    "            pivot_row['depth_bin_' + str(r.depth_bin)] = r.CHLA\n",
    "    df = pd.DataFrame(pivoted_ls[1:])\n",
    "    df.iloc[:, 10:] = IterativeImputer().fit_transform(df.iloc[:, 10:])\n",
    "    if save_path != None:\n",
    "        df.to_csv(save_path, index=False)\n",
    "    if return_df:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows lost on specific train test split merger w max normalized depth\n",
      "and merger with cluster data: 2374\n"
     ]
    }
   ],
   "source": [
    "# read in necessary data\n",
    "df_pivoted = pd.read_csv('../data/pivoted_imp_data.csv')\n",
    "df_clusters = pd.read_csv('../data/cluster_classification_assignment.csv', low_memory=False)\n",
    "df_train_xgb = pd.read_csv('../data/xgb_train_preds.csv')\n",
    "df_test_xgb = pd.read_csv('../data/xgb_test_preds.csv')\n",
    "df_split = pd.read_csv('../data/float_sat_normalized_depth_train_test_split.csv')\n",
    "\n",
    "def build_normalized_data(df_pivoted, df_clusters, df_train_xgb, df_test_xgb, n_pts=10, save_path=None, return_df=True):\n",
    "    # add train test tag for splitting later\n",
    "    df_train_xgb['train'] = 1\n",
    "    df_test_xgb['train'] = 0\n",
    "\n",
    "    # get information on max depth and train test split using Josie's data\n",
    "    df_norm_depth = pd.concat([df_train_xgb, df_test_xgb]).groupby(['float', 'cycle']).max().reset_index()\\\n",
    "        .rename(columns={'cycle': 'cycleNumber', 'PRES': 'max_depth'})[['float', 'cycleNumber', 'max_depth']]\n",
    "    # merge with pivoted data I created\n",
    "    df = df_pivoted.merge(df_norm_depth, on=['float', 'cycleNumber'])\n",
    "    df = df.merge(df_split[['float', 'cycleNumber', 'train']], on=['float', 'cycleNumber'])\n",
    "    # add clusters from Gabby's data\n",
    "    df = df.merge(df_clusters[['float', 'cycleNumber', 'cluster']],\n",
    "                                    on=['float', 'cycleNumber']).drop_duplicates()\n",
    "    print(f'Rows lost on specific train test split merger w max normalized depth\\nand merger with cluster data: {df_pivoted.shape[0] - df.shape[0]}')\n",
    "\n",
    "    # make an integer value representing the max bin to use in normalized data\n",
    "    df['max_bin'] = (np.floor(df.max_depth / 10) + 1).astype('int64')\n",
    "\n",
    "    norm_vals = {}\n",
    "    for i in range(n_pts):\n",
    "        norm_vals['norm_' + str(i)] = list()\n",
    "    # iterate through dataframe and find the CHLA values at normalized depths using interpolation\n",
    "    standard_depths = np.linspace(0, 1, n_pts)\n",
    "    for i, r in df.iterrows():\n",
    "        depth_chla = r[r.index.str.contains('depth_bin')].iloc[:r.max_bin + 1].to_numpy()\n",
    "        norm_depths = np.linspace(0, 1, len(depth_chla))\n",
    "        inter_fun = interp1d(norm_depths, depth_chla, kind='quadratic')\n",
    "        vals = inter_fun(standard_depths)\n",
    "        for i in range(n_pts):\n",
    "            norm_vals['norm_' + str(i)].append(vals[i])\n",
    "\n",
    "    # add the depths to the train test set\n",
    "    for i in range(n_pts):\n",
    "        df['norm_' + str(i)] = norm_vals['norm_' + str(i)]\n",
    "\n",
    "    # factorize clusters\n",
    "    cluster_nums, cluster_indeces = pd.factorize(df.cluster)\n",
    "    df['cluster_val'] = cluster_nums\n",
    "\n",
    "    if save_path != None:\n",
    "        df.to_csv(save_path, index=False)\n",
    "    if return_df:\n",
    "        return df, cluster_indeces\n",
    "dft, cs = build_normalized_data(df_pivoted, df_clusters, df_train_xgb, df_test_xgb, n_pts=11, save_path='../data/final_norm_set_11.csv', return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_known_dist = pd.read_csv('../data/cluster_chla_distribution.csv')\n",
    "\n",
    "def build_normalized_cluster_centers(df_known_dist, plot_clusters=False, return_centers=True):\n",
    "    out = pd.cut(df_known_dist.PRES, bins=25, labels=[i for i in range(25)])\n",
    "    df_known_dist['depth_bin'] = out\n",
    "    df_known_dist_binned = df_known_dist[['cluster', 'depth_bin', 'CHLA']] \\\n",
    "        .groupby(['cluster', 'depth_bin']).mean().reset_index().dropna()\n",
    "    xnew = np.linspace(0, 1, 25)\n",
    "    raw_dists = list()\n",
    "    norm_cluster_depths = {\n",
    "        'AR': 14,\n",
    "        'EQ': 18,\n",
    "        'HCB': 23,\n",
    "        'LCB': 23,\n",
    "        'PDCM': 23,\n",
    "        'SDCM': 20\n",
    "    }\n",
    "    if plot_clusters:\n",
    "        fig, ax = plt.subplots(2, 3, figsize=(20, 10), sharey=True)\n",
    "    for i, c in enumerate(np.unique(df_known_dist_binned.cluster)):\n",
    "        ydata = df_known_dist_binned[df_known_dist_binned.cluster == c]['CHLA'].to_numpy()[:norm_cluster_depths[c]]\n",
    "        xdata = np.linspace(0, 1, len(ydata))\n",
    "        fc = interp1d(xdata, ydata, kind='slinear')\n",
    "        ynew = fc(xnew)\n",
    "        raw_dists.append(ynew)\n",
    "        if plot_clusters:\n",
    "            ax[i // 3, i % 3].plot(xnew, ynew, label=c)\n",
    "            ax[i //3, i % 3].legend()\n",
    "    if return_centers:\n",
    "        return raw_dists"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c9ece6d5e3a61ef188a80f993fe2bc31561efaaa1bfc5da7794f01208afce4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
